\documentclass{report}

%------------------------------------------------------------------------------------------------------------------------
%Packages

\usepackage{float}
\usepackage{amsmath,amsfonts}
\newcommand{\R}{\mathbb{R}}
\usepackage{graphicx}
\usepackage{textcomp,gensymb}
\usepackage{color}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{a4wide}
\usepackage{colortbl}
\usepackage{hyperref}
\usepackage{wrapfig}
\usepackage{filecontents}
\usepackage[backend=bibtex,natbib=true, sorting=nyt,style=authoryear]{biblatex}

\bibliography{references}
%\addbibresource{references.bib}
\overfullrule=2cm             %To spot overfull hboxes
%------------------------------------------------------------------------------------------------------------------------

<<setup, include=FALSE, cache=FALSE, echo=FALSE>>=
library(ggplot2)
library(knitr)
library(data.table)
library(quantreg)
library(HourlyPrecipExtr)
# set global chunk options
opts_chunk$set(eval=TRUE, results = "hide",comment=FALSE,
               echo=FALSE, fig.height=5, fig.width=5, #message=FALSE,
               fig.pos="!ht", fig.align='center') #,tidy=TRUE
@

\title{Generic Report\\
        Method}
\author{Mendy van der Vliet}

\begin{document}

\maketitle


\chapter{Method}

\section{Introduction}
In this section an overview is given of the method involved in this work. Firstly, the meta data used and its relevance is described. Secondly, theory about linear trend analysis and quantile regression is discussed. Furthermore, the relevance of independent data in trend analysis is discussed. Moreover, methods of gaining independent data and testing for significance are given. Lastly, we elaborate on how we expanded the general statistical analysis in such a way relevant mechanisms can be investigated. This include Pearson and Spearmann correlations, multiple regression and regression of binned quantiles.

\subsection{Data}

\subsubsection{Time and place}
For the first part of this thesis rain gauge data on an hourly resolution is used from five stations in the Netherlands. Taking into consideration the need for having a long as possible consistent time serie as well as data which is spatially representative for different parts of the country, we were left with data of five stations. These stations are: de Kooy(235), De Bilt(260), Eelde(280), Vlissingen(310) and Maastricht(380), of which the location is shown in Figure \ref{fig:KaartStationsKNMI}. Coordinates and altitude of the stations are listed in Table \ref{tab:location}. For each station data is available on hourly resolution of which the following variables are used in this study: time, hourly precipitation sum, duration, temperature, dew point temperature, relative humidity, wind direction, wind strength, CAPE and weather code. As our focus of interest is precipitation extremes and their lifetime is often on the subhourly to hourly scale, we have chosen a time period by considering both a high resolution as well as a long enough time span, to be able to detect trends. For the period from 01-01-1958 till 31-12-2015 coherent time series for all stations are present, which can be easily compared and simultaneously analysed. Only CAPE data is more restricted in time and place. These CAPE observations are derived from radiosonde measurements in the period 01-03-1993 to 31-12-2015 at the Bilt, not at the other stations. In contrast to the other data, CAPE data also differs in resolution, from a mean of 4 measurements a day to 1 a day in the course of time. Days which were thought to be meteorological interesting, have a daily maximum of 5-6 CAPE measurements.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.7\linewidth]{/usr/people/vliet/Documents/HourlyPrecipExtr/figure/KaartStationsKNMI.png}
  \caption{A map of the Netherlands showing the locations of the five stations; de Kooy(235), De Bilt(260), Eelde(280), Vlissingen(310) and Maastricht(380).}
  \label{fig:KaartStationsKNMI}
\end{figure}

<<Nmean>>=
load("~/Documents/HourlyPrecipExtr/inst/tussenStap/Compare.rda")
Nmean <- Compare[,list(Nmean=unique(Nmean)),by=list(Year)]
@

\begin{figure}[ht]
  \centering
  \textbf{The resolution of CAPE observations}\par\medskip
  \begin{minipage}{0.48\textwidth}
 <<PointN_CAPE>>=
 # ggplot(Compare[1950:2050][sbCAPE >= 0])+geom_line(aes(x=date,y=sbCAPE))+ scale_x_datetime(date_labels = "%d %H:%M")+theme_bw()
 ggplot(Compare) + geom_point(aes(x=Date, y=N)) + theme_bw()
 @
     \caption{The number of observations a day, N}
     \label{fig:NCAPE}
  \end{minipage}
  \hfill
  \begin{minipage}{0.48\textwidth}
<<Nmean_CAPE>>=
ggplot(Nmean) + geom_bar(aes(x=Year,y=Nmean),stat="identity") + theme_bw()
@
     \caption{The yearly mean of N}
     \label{fig:NmeanCAPE}
  \end{minipage}
  \caption{The number of CAPE measurements a days showed as a point cloud in a, and as bars representing the yearly means in b.}
  \label{}
\end{figure}

\begin{table}[ht]
  \centering
  \caption{Station location and altitude}
  \begin{tabular}{l|r|r|r|r}
  \hline\hline
    Station   & STN & LON(east) & LAT(north) & ALT (m)\\
    \hline
    De Kooy   & 235 & 4.785     & 52.924     & 0.50 \\
    De Bilt   & 260 & 5.177     & 52.101     & 1.90 \\
    Eelde     & 280 & 6.586     & 53.125     & 3.50 \\
    Vlissingen& 310 & 3.596     & 51.442     & 8.00 \\
    Maastricht& 380 & 5.768     & 50.910     & 114.00 \\
     \hline
  \end{tabular}
  \label{tab:location}
\end{table}


\subsubsection{Type of data and instruments}
\label(subsubsec:typedata)

The variables investigated this thesis, and information about their unit, resolution and measurement instrument, are listed in Table \ref{tab:typeofdata}. Precipitation intensity is defined as the mean hourly intensity in mm hr$^{-1}$, so the accumulated rain amount divided by 1 hour. CAPE is defined as the amount of energy available for free convection, and computed as the amount of work done by the buoyancy force in a parcel of air that is lifted from the lifting condensation level to the level of no buoyancy \citep{Lin2007},
\begin{equation}
CAPE = \int^{z_{LNB}}_{z_{LFC}} g(\frac{T-\bar{T}}{\bar{T}})dz
\end{equation}.
The weather code is determined by considering the air and wet bulb temperature, meteorological sight, relative humidity, cloud coverage, precipitation type and intensity, wind velocity and number of lightning strikes within 0-15 and 15-20 km distance. For more information about the instruments involved and the exact conditions for every code, see \citet[Chapter 14]{KNMIhandboek2000}
\begin{table}[ht]
  \centering
  \caption{Data characterstics}
  \begin{tabular}{l|r|r|r}
    \hline\hline
    Name                            & Unit                  & Resolution      & Instrument  \\
    \hline
    Precipitation                   & Hourly sum in mm      & 0.1 mm          & Automatic rain gauge\\
                                    & (=liter m$^{-2}$)     &                 & \\
    Duration                        & part of the hour      & 0.1 hour        & Rain sensor\\
    Temperature                     & $^{\circ}$C           & 0.1 $^{\circ}$C & Electronic KNMI\\
                                    &                       &                 & Pt-500 sensor\\
    Dewpoint temperature            & $^{\circ}$C           & 0.1 $^{\circ}$C & KNMI Pt-500 and\\
                                    &                       &                 & E\&E33 sensor\\
    CAPE                            & J kg$^{-1}$           & 1 J kg$^{-1}$   & Radiosonde \\
    Relative humidity (1.5m hoogte) & \%                    & 1 \%            & E\&E33 sensor\\
    Wind strength                   & Hourly mean m/s       & 0.1 m/s         & AWS op 10m hoogte\\
    Wind direction                  & Mean of last 10 min.  & 1 degree        & AWS op 10m hoogte\\
                                    & of the hour, degrees  &                 & \\
    Weather code                    & Number 0-99           & n/a             & multiple instruments\\
    \hline
  \end{tabular}
  \label{tab:typeofdata}
\end{table}

\begin{wrapfigure}{R}{0.2\textwidth}
  \vspace{-15pt}
  \centering
  \includegraphics[width=0.18\textwidth]{/usr/people/vliet/Documents/HourlyPrecipExtr/figure/Handraingauge.png}
  \caption{Example of a manual gauge (with the rim at 0.4 m)}
  \label{fig:hand}
\end{wrapfigure}

Untill the beginning of the nineties all precipitation measurements were conducted manually with a pluviograph. This is an instrument which measures the amount of water fallen into the rain gauge with the help of a plastic graduated cylinder \citep[Chapter 6]{KNMIhandboek2000}. Between 1963 and 1981 the manual rain gauges of the different stations were replaced by gauges with an orifice area from 200 cm$^2$ instead of 400 cm$^2$, of with the effect is considered to be small \citet{Denkema1980,Denkema1981,Warmerdam1981,Brandsma2014}. Measurement incertainties of manual rain gauges may be caused by wind-induced transport of water droplets or snow flakes, evaporation of droplets or loss due to over pouring of water (to the graduated cylinder). These errors can be reduced by choosing a good environment \citep[Chapter 6]{KNMIhandboek2000}. Since 1991 (Eelde and Maastricht) or 1993 (De Kooy, De Bilt and Vlissingen) the rain measurements are automatized (\ref{tab:histchanges}). The instrument automatically registers the height of the float, which is attached to a potentiometer, representing the amount of fallen precipitation \citep[Chapter 6]{KNMIhandboek2000}. Any precipitation fallen in the solid phase is first melted by heating of the funnel, in order to be measured in the liquid phase. An illustration of the so-called KNMI rain gauge is given by Figure \ref{fig:automatic} (for more information see \citet{Wauben2004}). The common setup of an automatic measuring site is the English setup (Figure \ref{fig:englishset}). In a comparison study between manual and automatic networks of rain gauges \citet{Brandsma2014} concluded that automatic rain gauges measure 5-8\% less on annual basis (6.5\% averaged over the entire period 2001-2013).

\begin{figure}[ht]
  \centering
  \begin{minipage}{0.48\textwidth}
    \includegraphics[width=0.7\linewidth]{/usr/people/vliet/Documents/HourlyPrecipExtr/figure/Automatic_gauge.jpg}
    \caption{The design of an automatic rain gauge \citep[Chapter 6]{KNMIhandboek2000}.}
    \label{fig:automatic}
  \end{minipage}
  \hfill
  \begin{minipage}{0.48\textwidth}
\includegraphics[width=0.7\linewidth]{/usr/people/vliet/Documents/HourlyPrecipExtr/figure/English_setup.png}
     \caption{An automatic rain gauge of the KNMI in English setting \citep{Brandsma2014}.}
     \label{fig:englishset}
  \end{minipage}
\end{figure}

In English setup the gauge is placed on a small concrete box with a drainage tube in the middle of a circular wall with a diameter of 3 meters \citep{Brandsma2014}. The standard height of both the top of the rain gauge and the wall is 40 cm. See \citet{Brandsma2014} for futher details about the material and environment surrounding the pit. The English setup was implemented to prevent wind-induced loss, however in a study of \citet{Braak1945} this is contradicted. Currently, most rain gauges are not longer in the English setup, but situated in an Ott windscreen. Compared to the English set up \citet{Wauben2004} concluded that the Ott wind screen has a (extra) reduction in the annual mean precipitation of 1.5\% (to 6\% in windy conditions). Table \ref{tab:histchanges} summarizes the most important historical changes for the five stations regarding precipitation measurements (from \url{http://projects.knmi.nl/klimatologie/metadata/}). For further details about the measurements in temperature, dewpoint temperature, wind direction and wind speed, see \citet{KNMIhandbook2000}. For historical changes of the other meteorolical parameters than precipitation, go the KNMI site (\url{http://projects.knmi.nl/klimatologie/metadata/}). A classification of all weather codes can be found in Appendix A from \citet{Rulfova2013}. To differentiate between stratiform and convective precipitation we hold the same classification based on weather codes, as described in \citet{Rulfova2013}.

% minipage environment around tables
\begin{table}[ht]
  \centering
  \caption{Historical changes precipitation data}
  \begin{tabular}{|l|r|}
    \hline\hline
    \multicolumn{2}{|c|}{Orifice area reduction from 400 to 200cm$^2$} \\
    \hline
    Station     & When  \\
    \hline
    De Kooy     & 19930101\\
    De Bilt     & 19810401 \\
    Eelde       & 19730502 \\
    Vlissingen  & 19620410 \\
    Maastricht  & 19760701\\
    \hline
    \multicolumn{2}{|c|}{Automatic measurement}\\
    \hline
    Station     & When  \\
    \hline
    De Kooy     & 19720801 \\
    De Bilt     & 19930301 \\
    Eelde       & 19910316 \\
    Vlissingen  & 19930501 \\
    Maastricht  & 19910301 \\
    \hline
  \end{tabular}
  \label{tab:histchanges}
\end{table}

\begin{table}[ht]
  \centering
  \caption{Historical changes precipitation data}
  \begin{tabular}{|l|r|r|}
    \hline\hline
    \multicolumn{3}{|c|}{Ott windscreen}\\
    \hline
    De Kooy     & 20070426 &\\
    De Bilt     & 20080925 &\\
    Eelde       & 20090518 &\\
    Vlissingen  & - &\\
    Maastricht  & - &\\
    \hline
    \multicolumn{3}{|c|}{Relocation of instrument} \\
    \hline
    Station     & When & How much \\
    \hline
    De Kooy     & - & \\
    De Bilt     & 20080925, 13.00 UT & 200 m\\
    Eelde       & 19730501  & 750 m\\
    Vlissingen  & - & \\
    Maastricht  & 20051101 & 1770 m \\
    \hline
  \end{tabular}
  \label{tab:histchanges2}
\end{table}

For this study we applied rounding to the lowest hourly precipitation sums, and converted units from 0.1 mm and 0.1 $^{\circ}$C to 1 mm and 1 $^{\circ}$C, and sometimes selected data on a certain criterium. In order to have a consistent rouding of hourly precipitation sums to 0.1 mm, the hours with amounts classifed as $<$ 0.05 mm are treated as being zero in this study. This leads to more dry hours ($\pm$ 8\% of total amount of hours). For frequency analysis we could argue to include these hours and code them as wet, however for intensity analysis this is not desired. The exact amounts are namely not known. Therefore, we choose for a consistent data set, in which wet hours are defined as hours in which precipitation accumulates to amounts higher than 0.05 mm. In Chapter 1(REF CH1) we sometimes consider only wet hours, when we are interested in characterstics of rain events. In Chapter 3 we are interested in the correlation between precipitation and other variables. Therefore, when considering daily maxima, we only take into account wet days (days in which the total amount of precipitation is higher than zero). Another reason to investigate the statistics of wet events only, is that these are less dependent on changes in the atmospheric circulation \citep{Lenderink2011}, which can be handy when studying local factors such as moisture availibility and CAPE.

\subsubsection{Data quality}
\label{subsubsec:dataquality}
For trend analysis a high quality data set is desired, especially one whose variability consist solely of changes in weather and climate. This is the definition of an homogeneous climate time series \citep{Freitas2013}. Nonetheless, long instrumental records are rarely homogeneous, for example due to station relocations, changes of rain gauge type, and changes in the rain gaug site \citep{Buishand2013,Freitas2013}. As can be deduced from Table {tab:histchanges} our data sets also have inhomogeneities. Although much attention has been devoted to homogeneity testing and adjusting for inhomogeneities, methods thereof are not considered here for multiple reasons. First of all, inventing and applying homogenization tests for hourly data is a time consuming and difficult task and would not fit within the framework of this thesis. Secondly, there are no excisting homogenization programs available on hourly resolution to implement easily. Thirdly, homogenization tests in which neighbouring stations are used in pairwise comparison would not be suited here, due to the large spatial distance between the stations. Besides there are no other neigbouring stations matching in resolution and length of data set. A possible way of correcting for instrumental errors is by comparing the automatic rain gauges with the hand rain gauges, because we know from \citet{Brandsma2014} that the former measures annually 5-8\% lower than the latter \ref(subsubsec:typedata). This can be explained by errors in evaporation values due to a warming element in the automatic rain gauge and by a relatively less favorable aerodynamic shape \citep{STOWA2004}. However, there are several limitations to this correction. First, the timing of manual measurement is less thrustworthy, especially in case of extreme precipitation, resulting in large absolute differences. Second, the correction factor as applied by \citet{Buishand1988} (multiplying every hour by the factor, $f=D/P$, with D the daiy sum of the manual rain gauge and P of the automatic rain gauge) is not valid here. Rain is namely not evenly distributed over the hours, and this kind of errors will be bigger for extreme rain intensities. Third, the manual and automatic rain gauges at all sites are placed further from each other in the course of time (\ref{tab:histchanges}). The larger the distance and the more local the rain events of interest, the smaller the benefit of this correction technique. As intense convective rain events can be very local, this is a serious limitation. Last, an underestimation of automatic measurements is not that relevant for trends in extremes, when this is consistent in time. Then we can still deduce trends. This leads us to the conclusion that the proposed types of correction are not worthwhile to apply on our data. As we can deduce fromTable {tab:histchanges} some measurement errors change in time due to the implementation of an different kind of instrument or relocation, therefore these errors are not consistent over the entire time period. In order to study possible jumps in our precipitation data, we show the annual precipitation means in combination with the timing of the important historical changes for every station in Figure \ref{fig:mean_histchanges}.

\newpage

<<load_hrKNMI>>=
load("./inst/tussenStap/hrKNMI.rda")
@
\begin{figure}[ht]
  \centering
  \textbf{Relation between annual precipitation means and historical changes}\par\medskip
  \begin{minipage}{0.48\textwidth}
<<Kmean_histchanges,cache=FALSE,fig.height=2.5>>=
#De Kooy 1972 (to 200 cm2 orifice area), 1993(automatic) 2007 (Ott wind screen)
yrmean <- hrKNMI[, list(yrmean = mean(P)),by=list(Year,STN)]
ggplot(yrmean[STN==235]) + geom_line(aes(x=Year,y=yrmean),colour="lightgray") + geom_point(aes(x=Year,y=yrmean)) + geom_smooth(aes(x=Year,y=yrmean), colour="red") + geom_vline(xintercept=1972, colour="purple",linetype=2) + geom_vline(xintercept=1993, colour="green",linetype=2) + geom_vline(xintercept=2007, colour="blue",linetype=2) + theme_bw()
@
  \caption{De Kooy}
  \label{fig:Kmean_histchanges}
  \end{minipage}
  \hfill
  \begin{minipage}{0.48\textwidth}
<<Bmean_histchanges,cache=FALSE,fig.height=2.5>>=
#De Bilt 1981 (to 200 cm2 orifice area), 1993(automatic), 2008 (Ott wind screen and relocation)
ggplot(yrmean[STN==260]) + geom_line(aes(x=Year,y=yrmean),colour="lightgray") + geom_point(aes(x=Year,y=yrmean)) + geom_smooth(aes(x=Year,y=yrmean), colour="red") + geom_vline(xintercept=1981, colour="purple",linetype=2) + geom_vline(xintercept=1993, colour="green",linetype=2) + geom_vline(xintercept=2008, colour="blue")  + theme_bw()
@
  \caption{De Bilt}
  \label{fig:Bmean_histchanges}
  \end{minipage}
  \hfill
  \begin{minipage}{0.48\textwidth}
<<Emean_histchanges,cache=FALSE,fig.height=2.5>>=
#Eelde 1973 (to 200 cm2 orifice area and relocation) 1991 (automatic), 2009 (Ott wind screen)
ggplot(yrmean[STN==280]) + geom_line(aes(x=Year,y=yrmean),colour="lightgray") + geom_point(aes(x=Year,y=yrmean)) + geom_smooth(aes(x=Year,y=yrmean), colour="red") + geom_vline(xintercept=1973, colour="purple") + geom_vline(xintercept=1991, colour="green",linetype=2) + geom_vline(xintercept=2009, colour="blue",linetype=2)  + theme_bw()
@
  \caption{Eelde}
  \label{fig:Emean_histchanges}
  \end{minipage}
  \hfill
  \begin{minipage}{0.48\textwidth}
<<Vmean_histchanges,cache=FALSE,fig.height=2.5>>=
#Vlissingen 1962 (to 200 cm2 orifice area), 1993 (automatic)
ggplot(yrmean[STN==310]) + geom_line(aes(x=Year,y=yrmean),colour="lightgray") + geom_point(aes(x=Year,y=yrmean)) + geom_smooth(aes(x=Year,y=yrmean), colour="red") + geom_vline(xintercept=1962, colour="purple",linetype=2) + geom_vline(xintercept=1993, colour="green",linetype=2) + theme_bw()
@
  \caption{Vlissingen}
  \label{fig:Vmean_histchanges}
  \end{minipage}
  \hfill
  \begin{minipage}{0.48\textwidth}
<<Mmean_histchanges,cache=FALSE,fig.height=2.5>>=
#Maastricht 1976 (to 200 cm2 orifice area), 1991 (automatic),  2005 (relocation)
ggplot(yrmean[STN==380]) + geom_line(aes(x=Year,y=yrmean),colour="lightgray") + geom_point(aes(x=Year,y=yrmean)) + geom_smooth(aes(x=Year,y=yrmean), colour="red") + geom_vline(xintercept=1976, colour="purple",linetype=2) + geom_vline(xintercept=1991, colour="green",linetype=2) + geom_vline(xintercept=2005, colour="orange",linetype=2)  + theme_bw()
@
  \caption{Maastricht}
  \label{fig:Mmean_histchanges}
  \end{minipage}
  \caption{The annual means in precipitations of De Kooy (a), De Bilt (b), Eelde (c), Vlissingen (d), and Maastricht (e), from 1958 till 2015. The vertical dashed lines give the timings of important historical changes, the type of change is indicated by the color: orifice area reduction (purple), automatic measurement (green), Ott windscreen (blue), and relocation of the instrument (orange). When one of the changes occur simultanously with a relocation a normal line instead of dashede line is used.}
  \label{fig:mean_histchanges}
\end{figure}

For de Kooy three important historical changes have taken place, of which the timing is indicated in Figure \ref{fig:Kmean_histchanges} by dashed lines. The dashed purple line is linked to the reduction in the orifice area from 400 to 200cm$^2$ in 1972. In our figure this effect seems to be neglible, confirming what was stated in \ref(subsubsec:typedata). The dashed green line represents the change from manual to automatic (electric) measurements in 1993. In this figure it is hard to detect an evident jump in our data between the period before and after the change. The little bump in the year 1993 is not thought to be the result of the change, as we would except a long-term shift in our data. In other words, we are seeking for simultaneous occurence of an historical change with a breakpoint in the annual precipitation means. The dashed blue line in Figure \ref{fig:Kmean_histchanges} relates to the change of setup; instead of an English setup the rain gauge is since 2007 located in a Ott windscreen. This would lead to a mean annual difference of 1.5\% lower, which could have influenced the breakpoint in the smoothed regression fit around 2007.

Contrary to a large temporal jump in the year 1982, we do not spot a clear shift in the precipitation data of De Bilt for the reduction in orifice area (Figure \ref{fig:Bmean_histchanges}). The increase in the annual mean loss due to the change from manual to automatic rain gauge can not be decuded from Figure \ref{fig:Bmean_histchanges}. However, for the implementation of the Ott windscreen and simultaneous relocation (200 m) of the station in 2008, we again observe a breakpoint in the smoothed regression line. It is possible that this change in setup has had an effect on the precipitation measured, although keep in mind that the confidence interval increases at the same time.

None of the historical changes regarding precipitation measurements at Eelde can be linked to breakpoints in the regression of the yearly mean (\ref{fig:Emean_histchanges}). This is interesting regarding the change to the Ott windscreen setup, of which we thought it might have influenced the decreasing trend in yearly mean when studying Figures \ref{fig:Kmean_histchanges},\ref{fig:Bmean_histchanges}. This is apparently not a consistent relationship. Therefore, it is still possible that this decrease starting the early 2000s has another (f.e. climatic) reason. Note, that the uncertainity, expressed by the confidence band, is quite high for this time period and station.

Figure \ref{fig:Vmean_histchanges} demonstrates the yearly precipitation means and two historical changes for the station in Vlissingen. Both the change in orifice area, as well as the implementation of an autmatic rain gauge, can not be linked to a breakpoint in the regression of the annual means. What is more striking about Figure \ref{fig:Vmean_histchanges}, is the much less pronounced interdecadal pattern in the annual means in precipitation, compared to the other stations.

The reduction in orifice area (in 1976) and change to an automatic rain gauge (1991) in Maastricht, does not correspond to a clear shift or jump in the annual precipitation means (\ref{fig:Mmean_histchanges}). The relocation in 2005, which was the farthest in  distance compared to the other stations, goes hand in hand with a breakpoint. However, this is probably the same decrease starting the early 2000s as in the other stations, which indicates that the mean driver is not a instrument-related change.

Overall, the effects of changes in type of instrument, setting and/or location are not likely to have a caused a signicant jump in the precipitation data. Whether the observed interdecadal patterns in Figure \ref{fig:mean_histchanges} are linked to climatic changes (f.e. changes in temperature) will be further investigated in Chapter 3 (REF).

\section{Definition Extremes}
A couple of so-called \enquote*{extreme indices} are listed in the Fifth Assessment Report of the IPCC report, which are widely used in literature. These indices are based on either the probability of occurrence of given quantities or on absolute or percentage threshold exceedances (relative to the climatological reference period), but also some complex definitions about duration, intensity and persistence of extreme events are involved \citep{IPCC2013}. These extreme indices have been selected on basis of their robust statistical properties, their applicability across a wide range of climate types and extensive data availability over space and time. These indices reflect more ‘moderate’ extremes, as they do not include 1 in 100 year events, but events taking place as often as 5\% or 10\% of the time \citep{IPCC2013}.

The indices used in this study are expressions of events occuring $1-45$ times a year, so including \enquote{high} to \enquote{moderate} precipitation extremes. Accordingly, the annual number of events is large enough to apply meaningfull trend analysis on 58 yr time series. Extreme events are defined using the 75,90,95,99,99.9 quantiles, so hours with the highest $25-0.1$\% intensities. Quantiles are cutpoints partitioning a probability distribution into contiguous intervals, expressed as the values above which $100$-$\tau$ (where $\tau$ is f.e. 75\%, the 75\% quantile) of the highest values are situated. For the entire data set with all hourly values, the 99 and 99.9 \%-quantiles express \enquote{moderate} extremes occuring 10-85 times a year, whereas dry hours occuring $\pm$ 88\% of the time. However, we only apply full trend analysis on data consisting of 2-day maxima (the reason for this will be further explained in \ref{subsec:Interdependency_of_data}). The $75-99.9$ quantiles of this data set correspond to $1-45$ times a year and intensities of $>$ $2-24$ mm hr$^{-1}$ of accumulated precipitation per hour. Here, the 99 and 99.9 \% quantiles represents the \enquote{high} extremes, with frequencies of approximately $0.18$ and 2 times per year. The 75, 90, 95 \% quantiles belong to values occuring $\pm$ 10, 18 and 45 times a year. These indices are chosen such that the extremes selected have impacting intensities($>$ $1.6-6$ mm hr$^{-1}$ for \enquote{moderate} extremes and $>$ 8-50 mm/hr for \enquote{high} extremes), as well as a high enough occurence per year for trend analysis.

\section{Trends}

In Chapter 2 (REF to CH2) trend analysis is applied on intensity and frequency (wet hours a day) data for every station.


\subsection{Simple linear trend}

To detect and quantify historical climate trends  the linear component of the change over time is often estimated. The strengths and weaknesses of this approach are well understood as it is frequently and widely applied \citep{Storch1999,Wilks2006,IPCC2013}. In linear trend modelling the way in which the trend is dependent on the sampling distribution (Gaussian, lognormal or otherwise) and the residuals on the trend line, has to be considered carefully. Moreover, uncertainty and serial correlation in the data have to be taken into account \citep{Storch1999,Santer2008,IPCC2013}. Two different methods of linear regression are used in this study; fitting a linear regression model by the ordinary least square approach, and by linear quantile regression. The choice for a certain method is based on the sampling distribution and the type of result interested in.

\subsubsection{Least-square regression fitting}

The ordinary least square (OLS) method is a minimization function of the squares of the vertical distance, $\sum{(y_i - \beta_0 - \beta_1 x_i)^2}$ from  the  data points  to  the  regression  line citep{Leng2007}. Figure \ref{fig:LR} illustrates this. The slope of this regression $\hat{\beta_1}$ is given by  $ \frac{\sum{(x_i - \bar{x})(y_i - \bar{y})}}{\sum{(x_i -\bar{x})^2}} = \frac{S_{xy}}{S_yy}$. In a similar way the OLS estimate of X on Y minimizes the horizontal distance between the points and the regeression line. The latter is also named the reverse regression. Ordinary least regression is only applicable when only one of two variables is random citep{Leng2007}.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.7\linewidth]{/usr/people/vliet/Documents/HourlyPrecipExtr/figure/Linearregression.png}
  \caption{Ordinary least square regression, which minimize the vertical (a) or horizontal (b) distance between the data points and the regression line.}
  \label{fig:LR}
\end{figure}

(maybe add something about how confidence, gray band is calculated. Used in Figures \ref{fig:Mmean_histchanges})
% Confidence band; uncertainty are computed using ...
% (?? The 90\% confidence interval quoted is solely that arising from sampling uncertainty in estimating the trend.
% Parametric and other remaining uncertainties (Box 2.1), for which estimates are provided with some data sets, are not included in the trend estimates shown here, so that the same method can be applied to all data sets considered.)

\newpage
\subsubsection{Quantile regression}

Quantile regression can be viewed as an optimatization of a linear $\tau$-dependent fit to a certain data set, thereby estimating the $\tau$-th quantiles of the response variable. As described in \citet{Koenker1978} we can describe the minimization problem to obtain the $\tau$-th sample quantile of data serie $y$ as

\begin{equation} \label{eq:sample_quant}
  \min_{\beta\in\R} \sum_{t=1}^{T} \rho_{\tau} (x_s(t) - \beta)
\end{equation}

where $\rho_{\tau}(u) = u(\tau - I (u<0))$
To solve a linear quantile fit, we specify the $\tau$-th conditional quantile function as $Q_y(\tau|x) = x'\beta(\tau)$ and rewrite \ref{eq:sample_quant}

\begin{equation} \label{eq:cond_quant}
 \min_{\beta\in\R} \sum_{t=1}^{T} \rho_{\tau} (x_s(t) - x'\beta(\tau))
\end{equation}

To illustrate how quantile regression works we plotted Figure \ref{fig:QR_example}.

<<Example_DT,cache=TRUE>>=
#
dt <- data.table(x = c(1:100),y = rnorm(100))
@

\begin{figure}[H]
  \centering
  \textbf{Example quantile regression }\par\medskip
<<Example_QR,cache=FALSE,fig.height=2,fig.width=4>>=
ggplot(dt,aes(x=x,y=y)) +
  geom_point(colour="gray") +
  geom_point(data=dt[(y < quantile(y,probs=0.50)) & (y > (quantile(y,probs=0.25)))],aes(x,y),color="black") +
  geom_point(data=dt[(y < quantile(y,probs=0.75)) & (y > (quantile(y,probs=0.50)))],aes(x,y),color="blue") +
  geom_point(data=dt[y > (quantile(y,probs=0.75))],aes(x,y),color="green") +
   stat_quantile(formula= y~x,quantiles=0.25, aes(colour='black'))+
   stat_quantile(formula= y~x,quantiles=0.50, aes(colour='blue'))+
   stat_quantile(formula= y~x,quantiles=0.75, aes(colour='green'))+
   scale_colour_manual(name = 'Quantiles', values=c('black','blue','green'),labels = c('25%','50%','75%'))+
    theme_bw()
@
  \caption{For a random data set of 100 values for all x (numbers from 1 to 100) the colored lines (black-green) give quantile regression fits with respectively $\tau$ = 25, 50, 75\%. The dots in black give all values higher than the 25\%-quantile. Note, these dots are overplot by the blue dots which indicate all values higher than the 50\%-quantile. The latter dots are also overplot by the green dots giving all values higher than the 75\%-quantile.}
  \label{fig:QR_example}
\end{figure}

Complementing the least square method, quantile regression not only provides a more robust alternative for estimating the main tendency of the response, but also offers the possibility to investigate better the conditional distribution of the response \citep{Koenker2005}. Quantiles are more relevant for spatial comparison then counts of exceedance over thresholds \citep{KleinTank2003}, as they correspond to the same part of the distribution for all stations, Multiple studies use linear regression for changes in seasonal or annual quantiles. However, when high quantiles are calculated from small samples, these can be biased. Therefore, quantile regression is used to overcome this problem \citep{Wasko2014,Roth2015}.

% Complementing ... method in the use of estimating conditional mean models, quantile regression..

% From \citet{Wasko2014} In this paper, we highlight limitations in the binning approach and present quantile regression as an alternative to the above process. Quantile regression allows estimation of this scaling directly and, unlike binning, is unbiased with sample size.
% A Mann-Kendall test could also be applied to test the hypothesis of a monotonic trend in the seasonal or annual quantiles. Though, this approach does not visualize the trend itself. Although, linear modelling is broadly accepted, it is not flexible enough when working with long measurement records which encounter possible phases of stagnation. In this case monotone quantile regression can be used. ))

Quantile regression was performed using the R package \enquote{quantreg} \citep{Koenker2013}. The default method for computing quantile regression is a modified version of the Barrodale and Roberts algorithm for l1-regression, described in  detail in \citet{Koenker1987,Koenker1994}.For large data sets (n $> 10000$) we use Frisch-Newton interior point method (see \citet{Koenker2005} for more information) to calculate quantile regression.

%In Chapter 3 we used a binned approach, as the differences between intervals of temperature as so big the quantile regression fits are biased. For every 2 degrees bin a quantile is calculated and regression on these quantile points is applied. In order to remove biased in the binned approach, due to low sample size for certain bins, only quantile values of bins containing more than 100 measurements are considered.


% - Extra: Monotone quantile regression

\subsection{Time dependency of data} \label{subsec:Interdependency_of_data}

When serial correlation is present, the linear regression estimates may be potentially invalid and inconsistent \citep{CFAmanual}.\citet[pp.128]{Koenker2005} stated that the typical independent and identically distributed errors (IID) condition (in other words the condition that all random variables have the same probability distribution as the others and are mutually independent) is most of the time influencing the regression error terms for both the mean- and quantile regression. Therefore, it is important to correct for autocorrelation when applying quantile regression to detect a trend \citep{Huo2013}. Autocorrelation is serial correlation or cross-correlation of a signal with itself at different points in time. Partial autocorrelations are serial correlations with intermediate autocorrelations removed. Based on the autocorrelation function (acf) and partial autocorrelation function (pacf) repeating time-dependent patterns can be recognized. To have a time-independent data set a sampling spacing larger than the significant lags of these patterns can be chosen.

\subsection{Testing and significance}

Whether least square or quantile fits on the independent data are significant, has to be tested. First, we start with an hypothesis of stationarity against a linear trend. In the case of stationarity the fitted value (f.e. the quantile of interest) is constant over time, i.e.
\begin{equation}
H_0: Q_i == \beta
\end{equation}
with the time step i,
or
\begin{equation}
H_0: \alpha == 0
\end{equation}

This can be tested against the alternative that it is increasing or decreasing, i.e.
\begin{equation}
H_1: Q_{-n} \le Q_i \le Q_{n}
\end{equation}
or
\begin{equation}
H_1: Q_i = \alpha_0 + \alpha_1 * i, \alpha_1 > 0
\end{equation}
with n indicating the nth timestep.

% (( Testing: H0 hypothesis of linear trend against monotone increasing trend
% H0: Qi = alpha_0 + alpha_1 * i, alpha_1 > 0
% Against the one-sided montone trend alternative, i.e.
% Qi = alpha_0 + i *n ,
% with n as timestep. ))

Here, we test whether the slopes of the fitted regression lines deviate significantly from zero. In a 9999-Monte Carlo permutation test we calculate the slopes of 9999 permutated versions of one time serie, to compare with the observed slope of one single regression. The corresponding p-value is computed as $p= \frac{\sum \alpha_{obs} > \alpha_{perm}}{N}$, with $\alpha_{obs}$ the slopes of the observed values, $\alpha_{perm}$ the slopes of the permutated series and N the number of of permutations. By comparing the p-value to the 1\%,5\%,95\% and 99\% confidence levels in the distribution of all slopes, the observed regression fits can be assigned a significant negative, positive or non-significant trend. Thereby rejecting or accepting the $H_0$ hypothesis.

The reason why a permutation test is chosen, is because this is a non-parametric test. Parametric approaches (f.e. z-,t-, or F-test) assume that the data is normally distributed, while a non-parametric (f.e permutation test) can be applied without the assumption that the data follows a normal distribution (\citet{Srinivasan}). A permutation (randomization) test is a resampling and exact test (the latter means that it is not defined on basis of parametric assumptions and does not use approximate algorithms). A basic premise of the permutation test is that the observations are exchangeable under the null hypothesis (\citet{Srinivasan}), so there should be no difference in location or method of measurement between the data. Although one data set consist of measurements at only one location, we already discussed tiny changes in the measurement settings of that sites in 58 years. These historical changes appeared to be insignificant \ref{subsubsec:dataquality}, so our assumption holds.

A major disadvantage of permutation tests is that they can be very demanding in computational power, therefore it is worthwile to use a Monte carlo approach. With this approach we mean a N-times repeated random sampling (with an specific order), instead of calculating the slope of every possible order of a serie of almost 10940 observated values, which would give 10940! outcomes. As the data set consist of so many points, the chances to have an identical ordered serie in 9999-random sampling is extremely low.

% Frequency data -->  in stead of 58, 999-random


%
% \subsection{Extreme values}
% Distribution, implications,
% link to quantile regression

%
\section{Mechanism}

Not worked out yet; includes explanation correlations (Pearson, Spearman), regression of binned quantiles and multiple regression
% ?! only intensity?
% A comprehenisve study into the mechanism behind the observed trends is carried out in chapter 4. First, we start with a theoretical framework to find precipitation-related variables. Moreover, we define key variables based on their correlation matrices and plotting patterns (precipitation over key variable x). Furthermore, lead-lag relationships are investigated and multiple regression is applied. (?! quantile regression is applied  to focus on the extreme part of the precipitation distribution. Finally, on the variable(s) which are the most likely to cause changes in precipitation, quantile regression in time carried out. The similarity in type and magnitude of a detected trend determines the likelyhood of an explanatory mechanism.
%
% \subsection{Identification}
% To investigate whether and what kind of relationships exist between the variables in our metadata we follow three steps in order to get closer to the key variables of interest. Correlation matrices of the variables which are related to precipitation based on literature gives a first-hand overview of the strength of any possible linear relationships. Plotting of the most promising variables against precipitation differentiated on season gives more information about the type and seasonal-dependance of the relation, whether this is linear(non-linear) and for which spectrum the correlation is best. To determine whether the relationships show any causality and in which direction, the correlation for each lag is considered. At last, trend analysis on the variable(s) explaining precipitation changes is applied to verify whether these are responsible for the intensification of hourly precipitation extremes. (?! and frequency)
%
%
% \subsubsection{Pearson and Spearman correlation}
% The correlations of the variables which are related to precipitation based on literature are shown in a heat map
% all data vs max -> extremes
%
% What is Pearson
% %https://statistics.laerd.com/statistical-guides/pearson-correlation-coefficient-statistical-guide.php
%
% Non-normality
% % http://d-scholarship.pitt.edu/8056/1/Chokns_etd2010.pdf
% % http://link.springer.com.proxy.library.uu.nl/article/10.1007/s10459-010-9222-y/fulltext.html
% % --> also for non-normal data Pearson can be used.
%
% But difference between Pearson and Spearman says something about linearity vs monotone increase of trends
% % From:http://www.gene-quantification.org/artusi-corr-2002.pdf,
% % http://www.degruyter.com.proxy.library.uu.nl/dg/viewarticle.fullcontentlink:pdfeventlink/$002fj$002fquageo.2011.30.issue-2$002fv10117-011-0021-1$002fv10117-011-0021-1.pdf?t:ac=j$002fquageo.2011.30.issue-2$002fv10117-011-0021-1$002fv10117-011-0021-1.xml
% %http://support.minitab.com/en-us/minitab-express/1/help-and-how-to/modeling-statistics/regression/supporting-topics/basics/a-comparison-of-the-pearson-and-spearman-correlation-methods/
%
% additional info
% % http://www.statisticssolutions.com/correlation-pearson-kendall-spearman/
%
%
% How to make simple heatmaps, also Spearman:
% %http://www.sthda.com/english/wiki/ggcorrplot-visualization-of-a-correlation-matrix-using-ggplot2
% %http://proven-inconclusive.com/blog/spearman_correlation_heat_map_with_correlation_coefficient_and_significance_in_r.html
%
% \subsubsection{Plotting}
%
% In order to study better linearity/monoticity of the relationships, we just plot the maxima of the important variables against precipitation maxima. Differences in the number of measurements for each unit of a variable can influence the fit, therefore we consider the distribution of each variable. When needed we apply binning of the variable to reduce the influence of distribution. The number of observations per bin must be $>$ .... For each bin the mean and the 95\% quantiles is calculated and a line is fitted through all the means and quantiles. As only the maxima are considered, the 95\% quantiles represent \enquote{high} extremes with values till 25 mm hr{-1}. Furthermore, all the three plots per variable are differentiated on season.
%
%
%
% \subsubsection{Spatial differences explained?}
% ??!
%
% \subsection{Lead-lag relationships}
%
% correlation for every lag , frequency > ...
% for lag=0, looked at the delta P over delta variable, before and after max.
%
%
%
%
% \subsection{Quantile regression}
% ?? more than quantile regression for P data
%
%
% \subsection{}
%
% Various auto-regressive and moving average patterns leave distinctive footprints on the autocorrelation and partial autocorrelation functions.  Time-series analysis is more appropriate for data with autocorrelation than, say, multiple regression, for two reasons. The first is that there is explicit violation of the assumption of independence of errors. The errors are correlated due to the patterns over time in the data. Type I error rate is substantially increased if regression is used when there is autocorrelation. The second is that the patterns may either obscure or spuriously enhance the effect of an intervention unless accounted for in the model.


% % From: https://datajobs.com/data-science-repo/Time-Series-Analysis-Guide.pdf:
% cite as Time-Series  Analysis.[Internet]  [cited  2014  July  8].  Available  from: https://datajobs.com/data-science-repo/Time-Series-Analysis-Guide.pdf
% The first three steps in the analysis, identification, estimation, and diagnosis, are devoted to modeling the patterns in the data. The first step is identification in which autocorrelation functions (ACFs) and partial autocorrelation functions (PACFs) are examined to see which of the potential three patterns are present in the data.
%
% ARMA
% % From: https://datajobs.com/data-science-repo/Time-Series-Analysis-Guide.pdf:
% The model described in this chapter is auto-regressive, integrated, moving average, called an ARIMA (p, d, q) model. The auto-regressive element, p, represents the lingering effects of preceding scores. The integrated element, d, represents trends in the data, and the moving average element, q, represents the lingering effects of preceding random shocks. A big question is how lingering is lingering? That is, do you have to take into account just the previous score (or shock) or do you get a better model if you take into account two or more of the previous scores (or shocks)?
%
% The second step in modeling the series is estimation in which the estimated size of a lingering auto-regressive or moving average effect is tested against the null hypothesis that it is zero. The third step is diagnosis, in which residual scores are examined to determine if there are still patterns in the data that are not accounted for. Residual scores are the differences between the scores predicted by the model and the actual scores for the series. If all patterns are accounted for in the model, the residuals are random. In many applications of time series, identifying and modeling the patterns in the data are sufficient to produce an equation, which is then used to predict the future of the process. This is called forecasting, the goal of many applications of time series in the economic arena. However, often the goal is to assess the impact of an intervention. The intervention
%
%
%
%
%
%
% Multiple regression
% % A word of caution about using multiple regression techniques with time series data: because of the autocorrelation nature of time series, time series violate the assumption of independence of errors.  Type I error rates will increase substantially when autocorrelation is present.  Also, inherent patterns in the data may dampen or enhance the effect of an intervention; in time series analysis, patterns are accounted for within the analysis. From: \url{http://userwww.sfsu.edu/efc/classes/biol710/timeseries/timeseries1.htm}

\printbibliography

\end{document}
