\documentclass{report}

%------------------------------------------------------------------------------------------------------------------------
%Packages

\usepackage{float}
\usepackage{graphicx}
\usepackage{color}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{a4wide}
\usepackage{colortbl}
\usepackage{hyperref}
\usepackage{filecontents}
\usepackage[backend=bibtex,natbib=true, sorting=nyt,style=authoryear]{biblatex}


\bibliography{references}
%\addbibresource{references.bib}
\overfullrule=2cm             %To spot overfull hboxes
%------------------------------------------------------------------------------------------------------------------------


\title{Generic Report\\
        Method}
\author{Mendy van der Vliet}

\begin{document}

\maketitle



\chapter{Method}

\section{Introduction}
In this section an overview is given of the method involved in this work.  Firstly, the meta data used and its relevance is described. Secondly, theory about linear trend analysis and quantile regression is discussed. Furthermore, the relevance of independent data in trend analysis is discussed. Moreover, methods of gaining independent data and testing for significance are given. Lastly, we elaborate how we expanded the general statistical analysis in such a way relevant mechanisms can be investigated.
Data
For the first part of this thesis rain gauge data on an hourly resolution is used from five stations in the Netherlands.  Taking into consideration the need for having a long as possible consistent time serie as well as data which is spatially representative for different parts of the country, we were left with data from the following stations. de Kooy (260), De Bilt(280),  Eelde(), Vlissingen() and Maastricht(380), of which the location is shown in Figure … . For each station metadata is available of which the following variables are used in this work: time, hourly precipitation sum, temperature, dew point temperature, wind direction … To have coherent time series which are easy to be compared and simultaneous analysed, the timeframe 1957 till present.
CAPE; observations of CAPE from radiosonde at the Bilt … from .. to … is investigated, calculated as follows;….
- explain why from 1957-present (data availibility hourly), why these stations (why also daily)
Coherent time series of hourly values, spatially representative
Not homogenized. Why not:
High absolute differences;
correction of hourly values for automatic vs hand rain gauge;
- Hand in extreme wet events is sometimes measured too late (large absolute differences)
- Correction factor for every hour is not so valid.
- Underestimation of automatic is not that relevant for trends in extremes;
As the underestimation is consistent in time, we can still deduce an trend. Otherwise the timing of transition in correction/correction error has to be know. For all stations the automatic and hand rain gauge are placed farther from each other one point in time. For The Bilt this is 1994;
-Comparison trends between these periods ( period with certain correction  vs period with other deviation in correction) ; homogenized serie of de Bilt has a lot of “mistakes”.
- As hand and automatic rain gauge are placed farther from each other. Correction of automatic with hand is not valid anymore for local/ convective rain showers  (mostly (summer) extremes).

\section{Linear trend analysis}
\subsection{Indices}
General: Klein Tank and Können (2003) (http://www.weeralarm.nl/publications/fulltexts/i15200442016223665.pdf)
of the extremes. The indices of temperature and precipitation extremes in this study were selected from the list of climate change indices recommended by the World Meteorological Organization–Commission for Climatology (WMO–CCL) and the Research Programme on Climate Variability and Predictability (CLIVAR). The selected indices are expressions of events with return periods of 5–60 days. This means that the annual number of events is sufficiently large to allow for meaningful trend analysis in ;50 yr time series. Although the selected indices refer to events that may be called ‘‘soft’’ climate extremes, these indices have clear impact relevance.
Current work: Extreme events are defined using the 95, 99 and 99.9\% quantiles. For hourly values we only consider the 99 and 99.9\% quantiles(comparable to threshold exceedence values of 2 and 5 mm/hr. These indices are chosen to define an extreme wet hour as a result of considering both the frequency of a moderate extreme (occuring 1 or 0.1\% of the time), the corresponding intensities and having enough data per year for trend analysis. Compared to the analysis of only wet hours or rain events (defined as consecutive rain hours), where all dry data are discarded, studing of hourly values also includes the dry hours (0 mm/hr for ~ 88\% of the time). Therefore, we only look at the very extreme tail (99\% and 99.9\% quantiles) of the hourly values which correspond to extreme values of $> 2$mm/hr.
From IPCC, http://www.climatechange2013.org/images/report/WG1AR5_ALL_FINAL.pdf p. 179-180:
\subsection{Linear Trends}
Historical climate trends are frequently described and quantified by estimating the linear component of the change over time (e.g., AR4). Such linear trend modelling has broad acceptance and understanding based on its frequent and widespread use in the published research assessed in this report, and its strengths and weaknesses are well known (von Storch and Zwiers, 1999; Wilks, 2006). Challenges exist in assessing the uncertainty in the trend and its dependence on the assumptions about the sampling distribution (Gaussian or otherwise), uncertainty in the data, dependency models for the residuals about the trend line, and treating their serial correlation (Von Storch, 1999; Santer et al., 2008). The quantification and visualization of temporal changes are assessed in this chapter using a linear trend model that allows for firstorder autocorrelation in the residuals (Santer et al., 2008; Supplementary Material 2.SM.3). Trend slopes in such a model are the same as ordinary least squares trends; uncertainties are computed using an approximate method. The 90% confidence interval quoted is solely that arising from sampling uncertainty in estimating the trend. Structural uncertainties, to the extent sampled, are apparent from the range of estimates from different data sets. Parametric and other remaining uncertainties (Box 2.1), for which estimates are provided with some data sets, are not included in the trend estimates shown here, so that the same method can be applied to all data sets considered.
\subsection{Extreme values}
Distribution, implications,
link to quantile regression

\subsection{Quantiles and quantile regression}
- Explain what are quantiles (elaborate),
- Explain what is quantile regression
- Pro s quantile regression
As quantiles correspond to the same part of the distribution for all stations, they are more relevant for spatial comparison then counts of exceedance over thresholds (Klein Tank and Können 2003). Multiple studie s use linear regression for changes in seasonal or annual quantiles. However, when high quantiles are calculated from small samples these can be biased. Therefore, quantile regression is used to overcome this problem (Roth, 2015).  A Mann-Kendall test could also be applied to test the hypothesis of a monotonic trend in the seasonal or annual quantiles. Though, this approach does not visualize the trend itself. Although, linear modelling is broadly accepted, it is not flexible enough when working with long measurement records which encounter possible phases of stagnation.
The default method for computing quantile regression is a modified version of the Barrodale and Roberts algorithm for l1-regression, described in  detail in Koenker and d’Orey (1987,1994).
For large data sets (n > 10000) we use Frisch-Newton interior point method to calculate quantile regression.

- Extra: Monotone quantile regression

\subsection{Interdependency of data}
Explain what is it, why important for linear trend analysis

From: https://datajobs.com/data-science-repo/Time-Series-Analysis-Guide.pdf:
The model described in this chapter is auto-regressive, integrated, moving average, called an ARIMA (p, d, q) model. The auto-regressive element, p, represents the lingering effects of preceding scores. The integrated element, d, represents trends in the data, and the moving average element, q, represents the lingering effects of preceding random shocks. A big question is how lingering is lingering? That is, do you have to take into account just the previous score (or shock) or do you get a better model if you take into account two or more of the previous scores (or shocks)?
The first three steps in the analysis, identification, estimation, and diagnosis, are devoted to modeling the patterns in the data. The first step is identification in which autocorrelation functions (ACFs) and partial autocorrelation functions (PACFs) are examined to see which of the potential three patterns are present in the data. Autocorrelations are self-correlations of the series of scores with itself, removed one or more periods in time; partial autocorrelations are self-correlations with intermediate autocorrelations partialed out. Various auto-regressive and moving average patterns leave distinctive footprints on the autocorrelation and partial autocorrelation functions. When the time series is long, there are also tendencies for measures to vary periodically, called seasonality, periodicity, or cyclic in time-series jargon. For example, viral infections peak during the winter months, as do calories and alcohol consumed. Thus, seasonality is another form of autocorrelation frequently seen in data sets. Periodic variation occurs over shorter time periods as well. For example, quality of manufacturing differs over days of the week, peaking in mid-week. And calorie and alcohol consumption also increase over the weekend. These patterns are also identified using ACFs and PACFs and accounted for in the model. Time-series analysis is more appropriate for data with autocorrelation than, say, multiple regression, for two reasons. The first is that there is explicit violation of the assumption of independence of errors. The errors are correlated due to the patterns over time in the data. Type I error rate is substantially increased if regression is used when there is autocorrelation. The second is that the patterns may either obscure or spuriously enhance the effect of an intervention unless accounted for in the model. The second step in modeling the series is estimation in which the estimated size of a lingering auto-regressive or moving average effect is tested against the null hypothesis that it is zero. The third step is diagnosis, in which residual scores are examined to determine if there are still patterns in the data that are not accounted for. Residual scores are the differences between the scores predicted by the model and the actual scores for the series. If all patterns are accounted for in the model, the residuals are random. In many applications of time series, identifying and modeling the patterns in the data are sufficient to produce an equation, which is then used to predict the future of the process. This is called forecasting, the goal of many applications of time series in the economic arena. However, often the goal is to assess the impact of an intervention. The intervention
A word of caution about using multiple regression techniques with time series data: because of the autocorrelation nature of time series, time series violate the assumption of independence of errors.  Type I error rates will increase substantially when autocorrelation is present.  Also, inherent patterns in the data may dampen or enhance the effect of an intervention; in time series analysis, patterns are accounted for within the analysis. From: \url{http://userwww.sfsu.edu/efc/classes/biol710/timeseries/timeseries1.htm}
From \url{https://www.google.nl/url?sa=t&rct=j&q=&esrc=s&source=web&cd=3&ved=0ahUKEwic7NP69p_LAhXJYpoKHU6fAA4QFggsMAI&url=https%3A%2F%2Fwww.cfainstitute.org%2Flearning%2Fproducts%2Fpublications%2Finv%2FDocuments%2Fquantitative_chapter10.pptx&usg=AFQjCNG1xw0kJNkEfd7tTdJPtfW4MrcFoQ&sig2=avtIb9k9mAHA0I3rh5OF_Q&bvm=bv.115339255,d.bGs&cad=rja:}
Are the results of our trend model estimation valid?
Trend models, by their very construction, are likely to exhibit serial correlation.
In the presence of serial correlation, our linear regression estimates are inconsistent and potentially invalid.
Use the Durbin–Watson test to establish whether there is serial correlation in the estimated model.
If so, it may be necessary to transform our data or use other estimation techniques.
+ explanation about AR (see pdf)

\subsection{Autocorrelation and partial autocorrelation}
Blocks of length > lags, explain  method to make data independent
Options independent fit (no autoregression):
Option 1
On basis of acf/pacf choose period of high dependency → choose max value a 48hr, or a week , to have approx. independent variable
Then quantile regression
Maybe verify if residuals have still autocorrelation
Test of significance : take into account that a bit of dependency is still in there

Significance autocorrelation precipitation: 2 dagen fysisch argument; winter depressie blijft 2 dagen

Option 2
Make a regression fit
Examine ARIMA structure
Use maximum likelihood to simultaneously estimate the regression model using ARIMA for the residuals
Examine ARIMA structure of sample residuals from the model in step 3. If white noise is present, the model is complete. If not, continue to asjust the ARIMA model for the errors until the residuals are white noise.

Papers

\subsection{Testing and significance}
Testing: H0 hypothesis of Stationarity against linear trend
With QR we can test the null hypothesis that the quantile of interest is constant over time, i.e.
H0:  Qi == Beta
against the alternative that it is non-decreasing, i.e.
H1: Qi <= … <= Qn
with n representing the nth timestep.
( Testing: H0 hypothesis of linear trend against monotone increasing trend
H0: Qi = alpha_0 + alpha_1 * i, alpha_1 > 0
Against the one-sided montone trend alternative, i.e.
Qi = alpha_0 + i *n ,
with n as timestep. )
Significance: We apply a Monte Carlo permutation test to calculate the p-values needed to be able to reject or accept the H0 hypothesis. This test captures the spatio-temporal correlation structure of the data.
Significance test:
Define H0, H1
Choose test: Permutation test
Find test statistic distribution by Monte Carlo resampling
Compute p-value (test statistic)
Statistical decision (reject or accept H0)

Why permutation test: parametric approaches (f.e. z-, t- or F-test) assume that the data follow a normal distribution. Permutation test (randomization tests) can be applied without the assumption of normal distributed data. This approach is an resampling and exact test (what does this mean??).
((Elaborate: Under H0 (the null hypothesis ), some of the data are exchangeable.  We permute (rearrange) the data by shuffling their labels of treatments, and then calculate our T.S. on each permutation. The collection of
T.S. from the permuted data constructs the distribution under H0.  …. We can see from this case normalization is useful for deal with real data in which is not perfectly “exchangeable” (Voetnoot nr 15)

 The basic premise is to use only the assumption that it is possible that all of the treatment groups are equivalent, and that every member of them is the same before sampling began (i.e. the slot that they fill is not differentiable from other slots before the slots are filled). An important assumption behind a permutation test is that the observations are exchangeable under the null hypothesis. An important consequence of this assumption is that tests of difference in location (like a permutation t-test) require equal variance.   → Testing for 1 location each, so no location difference.
The major down-side to permutation tests are that they
Can be computationally intensive and may require "custom" code for difficult-to-calculate statistics. This must be rewritten for every case.  → therefore Monte Carlo
Are primarily used to provide a p-value. The inversion of the test to get confidence regions/intervals requires even more computation.
conclusion (which is either or ) is correct with probability at least as large as . ( will typically be chosen to be extremely small, e.g. 1/1000.) Stopping rules to achieve this have been developed[14] which can be incorporated with minimal additional computational cost.))

As Permutation test  … we use the Monte Carlo approach.  (Voetnoot nr 15)


Two side permutation test, as we want know whether a trend is increasing (so not stationary, or decreasing.
One side: beta (slope of trend) = 0 or non zero.

What is MC : http://www.jstor.org.proxy.library.uu.nl/stable/2346974?seq=1#page_scan_tab_contents

Monte Carlo experiments: Monte Carlo methods (or Monte Carlo experiments) are a broad class of computational algorithms that rely on repeated random sampling to obtain numerical results. They are often used in physical and mathematical problems and are most useful when it is difficult or impossible to use other mathematical methods. Monte Carlo methods are mainly used in three distinct problem classes:[1] optimization, numerical integration, and generating draws from aprobability distribution.
Poisson distribution: is a discrete probability distribution that expresses the probability of a given number of events occurring in a fixed interval of time and/or space if these events occur with a known average rate and independently of the time since the last event.[1] The Poisson distribution can also be used for the number of events in other specified intervals such as distance, area or volume. The Poisson distribution has the following properties: – The mean of the distribution is equal to μ . – The variance is also equal to μ.

In statistics, Markov chain Monte Carlo (MCMC) methods are a class of algorithms for sampling from a probability distribution based on constructing a Markov chain that has the desired distribution as its equilibrium distribution. The state of the chain after a number of steps is then used as a sample of the desired distribution. The quality of the sample improves as a function of the number of steps.


\end{document}
